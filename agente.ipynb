{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlzMWbKwLfrf"
      },
      "source": [
        "# Imersão Dev Agentes de IA Google"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw4Hf62PxNUt"
      },
      "source": [
        "## Aula 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fbymIX3V0vF"
      },
      "source": [
        "Instalando as dependências necessárias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXzkE2XZIKHo"
      },
      "outputs": [],
      "source": [
        "# -q pra esconder os textos\n",
        "%pip install -q --upgrade langchain langchain-google-genai google-generativeai\n",
        "%pip install -q --upgrade dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHYgAluNTplG"
      },
      "source": [
        "Importando modelo e a chave da API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WRlObB2Le2p",
        "outputId": "c9a22c68-88c8-4831-b96e-55a7b4d59e0b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "try:\n",
        "  from google.colab import userdata # type: ignore\n",
        "  GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "  print(\"Rodando no Google Colab. Chave da API carregado de userdata.\")\n",
        "except ImportError:\n",
        "\n",
        "  load_dotenv('.env')\n",
        "  GOOGLE_API_KEY = os.getenv('GEMINI_API_KEY')\n",
        "\n",
        "  if GOOGLE_API_KEY is not None:\n",
        "    print(\"Rodando localmente. Chave da API carregada de arquivo .env.\")\n",
        "  else:\n",
        "    raise ValueError(\"GEMINI_API_KEY não encontrada.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD6JMbGqUi6S"
      },
      "source": [
        "Conexão com Gemini."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MFtgd7WRTvz"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model='gemini-2.5-flash',\n",
        "    temperature=0.0,\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dRYlEPuVm44"
      },
      "source": [
        "Aqui podemos fazer um teste de temperatura do modelo para a seguinte pergunta. Pode ser testado, por exemplo, o modelo com as temperaturas 0.0 e 1.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tTb0lgzVJ53",
        "outputId": "a90dbf0a-1e2e-4a3e-8abe-bce1b51fc74a"
      },
      "outputs": [],
      "source": [
        "resposta_teste = llm.invoke(\"Quem é você? Seja criativo!\")\n",
        "print(resposta_teste.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKPE4PzAvFJs"
      },
      "source": [
        "Um prompt para o nosso modelo de triagem. Seria uma mensagem que o próprio sistema passa para o modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BD7oZgjGYSCg"
      },
      "outputs": [],
      "source": [
        "TRIAGEM_PROMPT = (\n",
        "    \"Você é um triador de Service Desk para políticas internas da empresa Carraro Desenvolvimento. \"\n",
        "    \"Dada a mensagem do usuário, retorne SOMENTE um JSON com:\\n\"\n",
        "    \"{\\n\"\n",
        "    '  \"decisao\": \"AUTO_RESOLVER\" | \"PEDIR_INFO\" | \"ABRIR_CHAMADO\",\\n'\n",
        "    '  \"urgencia\": \"BAIXA\" | \"MEDIA\" | \"ALTA\",\\n'\n",
        "    '  \"campos_faltantes\": [\"...\"]\\n'\n",
        "    \"}\\n\"\n",
        "    \"Regras:\\n\"\n",
        "    '- **AUTO_RESOLVER**: Perguntas claras sobre regras ou procedimentos descritos nas políticas (Ex: \"Posso reembolsar a internet do meu home office?\", \"Como funciona a política de alimentação em viagens?\").\\n'\n",
        "    '- **PEDIR_INFO**: Mensagens vagas ou que faltam informações para identificar o tema ou contexto (Ex: \"Preciso de ajuda com uma política\", \"Tenho uma dúvida geral\").\\n'\n",
        "    '- **ABRIR_CHAMADO**: Pedidos de exceção, liberação, aprovação ou acesso especial, ou quando o usuário explicitamente pede para abrir um chamado (Ex: \"Quero exceção para trabalhar 5 dias remoto.\", \"Solicito liberação para anexos externos.\", \"Por favor, abra um chamado para o RH.\").'\n",
        "    \"Analise a mensagem e decida a ação mais apropriada.\"\n",
        "\n",
        "    \"\"\"\n",
        "    decisao: AUTO_RESOLVER,\n",
        "    urgencia: BAIXA,\n",
        "    campos_faltantes: []\n",
        "    \"\"\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MffnwGKyvP9h"
      },
      "source": [
        "Criação de uma estrutura de dados/formatação para o modelo de triagem utilizar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJXiDK70ZV97"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal, List, Dict\n",
        "\n",
        "class TriagemOut(BaseModel):\n",
        "  decisao: Literal[\"AUTO_RESOLVER\",\"PEDIR_INFO\",\"ABRIR_CHAMADO\"]\n",
        "  urgencia: Literal[\"BAIXA\", \"MEDIA\", \"ALTA\"]\n",
        "  campos_faltantes: List[str] = Field(default_factory=list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGKW6SWIvscn"
      },
      "source": [
        "Criação do modelo de triagem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Id_mBmKjayIS"
      },
      "outputs": [],
      "source": [
        "llm_triagem = ChatGoogleGenerativeAI(\n",
        "    model='gemini-2.5-flash',\n",
        "    temperature=0.0,\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKw-T0NQv3U4"
      },
      "source": [
        "Criação do método de triagem, unindo mensagem de usuário e mensagem do sistema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7WvbVr3bO_6"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "triagem_chain = llm_triagem.with_structured_output(TriagemOut)\n",
        "\n",
        "def triagem(mensagem:str) -> Dict:\n",
        "  saida: TriagemOut = triagem_chain.invoke(\n",
        "    [\n",
        "      SystemMessage(content=TRIAGEM_PROMPT),\n",
        "      HumanMessage(content=mensagem)\n",
        "    ],\n",
        "  )\n",
        "\n",
        "  return saida.model_dump()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaJxbZ0TvgSw"
      },
      "source": [
        "E aqui nós criamos algumas mensagens que o usuário pode mandar para o modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuVXQGqgdeUL"
      },
      "outputs": [],
      "source": [
        "testes = [\n",
        "    \"Posso reembolsar a internet?\",\n",
        "    \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "    \"Será que eu posso reembolsar recursos ou treinamentos da Alura?\",\n",
        "    \"Quantas capivaras tem no Rio Pinheiros?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzn4DKYRvf7O"
      },
      "source": [
        "Executar o teste:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "dCoMTvyad61u",
        "outputId": "ea1af48e-4463-4f05-ae1f-43fc62f6d072"
      },
      "outputs": [],
      "source": [
        "for msg_teste in testes:\n",
        "  print(f\"Pergunta: {msg_teste}\\n-> Resposta: {triagem(msg_teste)}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vWBP9yOxR92"
      },
      "source": [
        "## Aula 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbPsrWGwJnGL"
      },
      "source": [
        "*   Pymupdf: Ler pdfs\n",
        "*   Langchain_text_splitters: Separar o texto em pedaços menores\n",
        "*   Faiss CPU:\n",
        "*   Langchain_community: Separar o texto em pedaços menores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVoyMtwbxr5T"
      },
      "outputs": [],
      "source": [
        "%pip install -q --upgrade langchain_community faiss_cpu langchain_text_splitters pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP0g7xyHLp0D"
      },
      "source": [
        "Para essa aula, serão necessários 3 pdfs. Eles devem ser importados para esse ambiente (para que o modelo venha acessá-los)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42UNmGoGMHVc"
      },
      "source": [
        "Leitura dos pdfs:\n",
        "\n",
        "*   Política de Reembolsos (Viagens e Despesas).pdf\n",
        "*   Política de Uso de E-mail e Segurança da Informação.pdf\n",
        "*   Políticas de Home Office.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yNE-B7FL4-x",
        "outputId": "5277c287-f346-4e1a-df22-f071079f9e82"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "docs = []\n",
        "\n",
        "try:\n",
        "    import google.colab # type: ignore\n",
        "    pdf_path = Path('/content/')\n",
        "\n",
        "    print(\"Rodando no Google Colab. Usano pasta '/content/' para acessar pdfs.\")\n",
        "except ImportError:\n",
        "    # Assuming the PDFs are in a 'data' folder in the local environment\n",
        "    pdf_path = Path('./arquivos/')\n",
        "    print(\"Rodando localmente. Usando pasta './arquivos' para acessar pdfs.\")\n",
        "\n",
        "for doc in pdf_path.glob('*.pdf'):\n",
        "  try:\n",
        "    loader = PyMuPDFLoader(str(doc))\n",
        "    docs.extend(loader.load())\n",
        "    print(f\"Carregado arquivo '{doc.name}' com sucesso!\")\n",
        "  except Exception as e:\n",
        "    print(f\"Erro ao carregar arquivo '{doc.name}': Erro {e}\")\n",
        "\n",
        "print(f'Total documentos carregados: {len(docs)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7s-ed54OgdD"
      },
      "source": [
        "Dividindo o documento em 'chunks'. A separação não é bruta, pois há um \"overlap\" para que os chunks não fiquem \"quebrados no meio\", perdendo o sentido/contexto.\n",
        "\n",
        "Também é possível fazer essa separação manualmente (Ex.: usando pandas para separar por parágrafos cada chunk)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeDuW8byOPDn"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Tamanho \"padrão\" do chunk_size e chunk_overlap. 10% para o overlap\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
        "\n",
        "chunks = splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA62hSIeQNa0"
      },
      "source": [
        "Conteúdo do chunk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCVIx1HgPBrP",
        "outputId": "31a2ca85-99a7-48e1-cd43-c64a2956b003"
      },
      "outputs": [],
      "source": [
        "print(f'Total de chunks: {len(chunks)}\\n')\n",
        "\n",
        "print('------------------------------------------------------------------------------------------')\n",
        "for chunk in chunks:\n",
        "  print(f\"{chunk.page_content}\\n\")\n",
        "  print('-------------------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGCbNz_BRH9M"
      },
      "source": [
        "Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BflPMgiDT3YC"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/gemini-embedding-001\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIhjuj9QUrR-"
      },
      "outputs": [],
      "source": [
        "from re import search\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "# 0.3 é um padrão\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\",\n",
        "    search_kwargs={\"score_threshold\": 0.3, \"k\": 4}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui0bby95RLw4"
      },
      "source": [
        "Sugestão de atividade dos instrutores: mudar o prompt to sistema (Ex.: mudar nome da empresa, simular respostas)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUgEVCb4VKdg"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "prompt_rag = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",\n",
        "        \"Você é um Assistente de Políticas Internas (RH/IT) da empresa Carraro Desenvolvimento. \"\n",
        "        \"Responda SOMENTE com base no contexto fornecido. \"\n",
        "        \"Se não houver base suficiente, responda apenas 'Não sei'.\"),\n",
        "\n",
        "        (\"human\", \"Pergunta: {input}\\n\\nContexto:\\n{context}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "document_chain = create_stuff_documents_chain(\n",
        "    llm_triagem,\n",
        "    prompt_rag\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWuudvZ1YkUQ"
      },
      "source": [
        "Formatador no terminal (para ficar mais lindo e apresentável."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tO9vuGNkYje1"
      },
      "outputs": [],
      "source": [
        "# Formatadores\n",
        "import re, pathlib\n",
        "\n",
        "def _clean_text(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
        "\n",
        "def extrair_trecho(texto: str, query: str, janela: int = 240) -> str:\n",
        "    txt = _clean_text(texto)\n",
        "    termos = [t.lower() for t in re.findall(r\"\\w+\", query or \"\") if len(t) >= 4]\n",
        "    pos = -1\n",
        "    for t in termos:\n",
        "        pos = txt.lower().find(t)\n",
        "        if pos != -1: break\n",
        "    if pos == -1: pos = 0\n",
        "    ini, fim = max(0, pos - janela//2), min(len(txt), pos + janela//2)\n",
        "    return txt[ini:fim]\n",
        "\n",
        "def formatar_citacoes(docs_rel: List, query: str) -> List[Dict]:\n",
        "    cites, seen = [], set()\n",
        "    for d in docs_rel:\n",
        "        src = pathlib.Path(d.metadata.get(\"source\",\"\")).name\n",
        "        page = int(d.metadata.get(\"page\", 0)) + 1\n",
        "        key = (src, page)\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        cites.append({\"documento\": src, \"pagina\": page, \"trecho\": extrair_trecho(d.page_content, query)})\n",
        "    return cites[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpgcQ-pHSmZV"
      },
      "outputs": [],
      "source": [
        "def perguntar_politica_RAG(pergunta: str) -> Dict:\n",
        "  docs_relacionados = retriever.invoke(pergunta)\n",
        "\n",
        "  # Caso não houver nenhum documento, a gente nem para o modelo mandaremos\n",
        "  if not docs_relacionados:\n",
        "    return {\n",
        "        \"answer\": \"Não sei.\",\n",
        "        \"citacoes\": [],\n",
        "        \"contexto_encontrado\": False\n",
        "    }\n",
        "\n",
        "  answer = document_chain.invoke({\n",
        "      \"input\": pergunta,\n",
        "      \"context\": docs_relacionados\n",
        "    })\n",
        "\n",
        "  txt = (answer or \"\").strip()\n",
        "\n",
        "  # Caso ele já responder não sei, já retornamos uma resposta de \"Não sei\"\n",
        "  if txt.rsplit(\".!?\") == \"Não sei\":\n",
        "    return {\n",
        "        \"answer\": \"Não sei.\",\n",
        "        \"citacoes\": [],\n",
        "        \"contexto_encontrado\": False\n",
        "    }\n",
        "\n",
        "  return {\n",
        "    \"answer\": txt,\n",
        "    \"citacoes\": formatar_citacoes(docs_relacionados, pergunta),\n",
        "    \"contexto_encontrado\": True\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4FLQpRmWQHK"
      },
      "outputs": [],
      "source": [
        "testes = [\n",
        "    \"Posso reembolsar a internet?\",\n",
        "    \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "    \"Será que eu posso reembolsar recursos ou treinamentos da Alura?\",\n",
        "    \"Quantas capivaras tem no Rio Pinheiros?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2jL36RbWEW2"
      },
      "outputs": [],
      "source": [
        "for msg_teste in testes:\n",
        "    resposta = perguntar_politica_RAG(msg_teste)\n",
        "    print(f\"PERGUNTA: {msg_teste}\")\n",
        "    print(f\"RESPOSTA: {resposta['answer']}\")\n",
        "    if resposta['contexto_encontrado']:\n",
        "        print(\"CITAÇÕES:\")\n",
        "        for c in resposta['citacoes']:\n",
        "            print(f\" - Documento: {c['documento']}, Página: {c['pagina']}\")\n",
        "            print(f\"   Trecho: {c['trecho']}\")\n",
        "        print(\"------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRK_HI9XbTah"
      },
      "source": [
        "## Aula 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsSxzZwdbd9n"
      },
      "source": [
        "Instalando LangGraph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI5UOHHpbU0R"
      },
      "outputs": [],
      "source": [
        "%pip install -q --upgrade langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RA7YN1YXbkro"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Optional\n",
        "\n",
        "class AgentState(TypedDict, total=False):\n",
        "  pergunta: str\n",
        "  triagem: Dict\n",
        "  resposta: Optional[str]\n",
        "  citacoes: List[dict]\n",
        "  rag_sucesso: bool\n",
        "  acao_final: str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf0F5fNmhUzF"
      },
      "source": [
        "Criação das funções que representam os nós do agente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKij7jb5fHQL"
      },
      "outputs": [],
      "source": [
        "def node_triagem(state: AgentState) -> AgentState:\n",
        "  print(\"Executando nó de triagem...\")\n",
        "  resultado_triagem = triagem(state[\"pergunta\"])\n",
        "  return {\n",
        "    \"triagem\": resultado_triagem\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzoS8-OWfpM1"
      },
      "outputs": [],
      "source": [
        "def node_auto_resolver(state: AgentState) -> AgentState:\n",
        "  print(\"Executando nó de auto resolver...\")\n",
        "\n",
        "  resposta_rag = perguntar_politica_RAG(state[\"pergunta\"])\n",
        "\n",
        "  update: AgentState = {\n",
        "      \"resposta\": resposta_rag.get(\"answer\"), # Corrected key\n",
        "      \"citacoes\": resposta_rag.get(\"citacoes\",[]), # Corrected key and added default value\n",
        "      \"rag_sucesso\": resposta_rag.get(\"contexto_encontrado\"), # Corrected key\n",
        "  }\n",
        "\n",
        "  if resposta_rag.get(\"contexto_encontrado\"): # Corrected key\n",
        "    update[\"acao_final\"] = \"AUTO_RESOLVER\"\n",
        "\n",
        "  return update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHNp8_Cpf9dv"
      },
      "outputs": [],
      "source": [
        "def node_pedir_info(state: AgentState) -> AgentState:\n",
        "  print(\"Executando nó de pedir informações...\")\n",
        "\n",
        "  faltantes = state[\"triagem\"].get(\"campos_faltantes\",[])\n",
        "\n",
        "  detalhe = \",\".join(faltantes) if faltantes else \"Tema e contexto específico\"\n",
        "\n",
        "  return {\n",
        "      \"resposta\": f\"Para avançar, preciso que detalhe: {detalhe}\",\n",
        "      \"citacoes\": [],\n",
        "      \"acao_final\": \"PEDIR_INFO\"\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQ73mDMBisR4"
      },
      "outputs": [],
      "source": [
        "def node_abrir_chamado(state: AgentState) -> AgentState:\n",
        "  print(\"Executando nó de abrir chamado...\")\n",
        "\n",
        "  triagem = state[\"triagem\"]\n",
        "\n",
        "  return {\n",
        "      \"resposta\": f\"Abrindo chamado com urgência {triagem['urgencia']}. Descrição: {state['pergunta'][:140]}\",\n",
        "      \"acao_final\": \"ABRIR_CHAMADO\",\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zM_dKY8KlKwS"
      },
      "outputs": [],
      "source": [
        "# Dá para colocar mais palavras ainda\n",
        "# Serve como uma ajuda para a LLM\n",
        "KEYWORDS_ABRIR_TICKET = [\"aprovação\", \"exceção\", \"liberação\", \"abrir ticket\", \"abrir chamado\", \"acesso especial\"]\n",
        "\n",
        "def decidir_pos_triagem(state: AgentState) -> str:\n",
        "    print(\"Decidindo após a triagem...\")\n",
        "    decisao = state[\"triagem\"][\"decisao\"]\n",
        "\n",
        "    match decisao:\n",
        "      case \"AUTO_RESOLVER\":\n",
        "        return \"auto\"\n",
        "      case \"PEDIR_INFO\":\n",
        "        return \"info\"\n",
        "      case \"ABRIR_CHAMADO\":\n",
        "        return \"chamado\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6HBqDmqIFn6"
      },
      "outputs": [],
      "source": [
        "def decidir_pos_auto_resolver(state: AgentState) -> str:\n",
        "  print(\"Decidindo após o auto resolver...\")\n",
        "\n",
        "  if state.get(\"rag_sucesso\"):\n",
        "    print(\"Rag com sucesso, finalizando o fluxo...\\n\")\n",
        "    return \"ok\"\n",
        "\n",
        "  state_da_pergunta = (state[\"pergunta\" or \"\"]).lower()\n",
        "\n",
        "  if any(k in state_da_pergunta for k in KEYWORDS_ABRIR_TICKET):\n",
        "    print(\"Rag falhou, mas palavras-chave encontradas. Redirecionando para abertura de ticket.\")\n",
        "    return \"abrir_chamado\"\n",
        "\n",
        "  print(\"Rag falhou, vou pedir mais informações...\")\n",
        "  return \"pedir_info\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0f6q9pMKoIo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"triagem\", node_triagem)\n",
        "workflow.add_node(\"auto_resolver\", node_auto_resolver)\n",
        "workflow.add_node(\"pedir_info\", node_pedir_info)\n",
        "workflow.add_node(\"abrir_chamado\", node_abrir_chamado)\n",
        "\n",
        "workflow.add_edge(START, \"triagem\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"triagem\",\n",
        "    decidir_pos_triagem,\n",
        "    {\n",
        "      \"auto\": \"auto_resolver\",\n",
        "      \"info\": \"pedir_info\",\n",
        "      \"chamado\": \"abrir_chamado\"\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"auto_resolver\",\n",
        "    decidir_pos_auto_resolver,\n",
        "    {\n",
        "      \"info\": \"pedir_info\",\n",
        "      \"chamado\": \"abrir_chamado\",\n",
        "      \"ok\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"pedir_info\", END)\n",
        "workflow.add_edge(\"abrir_chamado\", END)\n",
        "\n",
        "grafo = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "eP5KtJjENHiS",
        "outputId": "1e2fc33c-57e2-4dd0-c37e-2e4c17d36b1e"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Image\n",
        "\n",
        "graph_bytes = grafo.get_graph().draw_mermaid_png()\n",
        "display(Image(graph_bytes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gb9SEg3fNZwu"
      },
      "outputs": [],
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "          \"É possível reembolsar certificações do Google Cloud?\",\n",
        "          \"Posso obter o Google Gemini de graça?\",\n",
        "          \"Qual é a palavra-chave da aula de hoje?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HrZU12ZqOIPZ",
        "outputId": "257d4c58-6624-4e30-d32b-0de42c47f0db"
      },
      "outputs": [],
      "source": [
        "for msg_test in testes:\n",
        "    resposta_final = grafo.invoke({\"pergunta\": msg_test})\n",
        "\n",
        "    triag = resposta_final.get(\"triagem\", {})\n",
        "    print(f\"PERGUNTA: {msg_test}\")\n",
        "    print(f\"DECISÃO: {triag.get('decisao')} | URGÊNCIA: {triag.get('urgencia')} | AÇÃO FINAL: {resposta_final.get('acao_final')}\")\n",
        "    print(f\"RESPOSTA: {resposta_final.get('resposta')}\")\n",
        "    if resposta_final.get(\"citacoes\"):\n",
        "        print(\"CITAÇÕES:\")\n",
        "        for citacao in resposta_final.get(\"citacoes\"):\n",
        "            print(f\" - Documento: {citacao['documento']}, Página: {citacao['pagina']}\")\n",
        "            print(f\"   Trecho: {citacao['trecho']}\")\n",
        "\n",
        "    print(\"------------------------------------\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
